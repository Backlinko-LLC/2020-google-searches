---
title: "Keyword landscape analysis - ahrefs"
author: ""
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    theme: paper
    highlight: kate
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
editor_options: 
  chunk_output_type: inline
---


<style>
.list-group-item.active, .list-group-item.active:hover, .list-group-item.active:focus {
  background-color: #00d188;
  border-color: #00d188;
}

body {
  font-family: montserrat;
  color: #444444;
  font-size: 14px;
}

h1 {
  font-weight: bold;
  font-size: 28px;
}

h1.title {
  font-size: 30px;
  color: #00d188;
}

h2 {
  font-size: 24px;
}

h3 {
  font-size: 18px;
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F, fig.path = "../plots/", cache = F, fig.showtext = TRUE, dpi = 700)

knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",", small.mark = ",", scientific = F)
})

Sys.setlocale("LC_TIME", "C")
extrafont::loadfonts(device = "win")
```


```{r prep}
set.seed(2)

required_packages <- c("tidyverse", "magrittr", "DBI", "bigrquery", "arrow","glue", "vroom","janitor", "gt", "ggwordcloud", "readxl", "ggthemes", "hrbrthemes", "extrafont", "plotly", "scales", "stringr", "gganimate", "here", "tidytext", "sentimentr", "scales", "DT", "here", "sm", "mblm", "glue", "fs", "knitr", "rmdformats", "janitor", "urltools", "colorspace", "pdftools", "showtext", "pander", "ggridges", "spatstat", "broom")
for(i in required_packages) { 
  if(!require(i, character.only = T)) {
    #  if package is not existing, install then load the package
    install.packages(i, dependencies = T)
  require(i, character.only = T)
  }
}

panderOptions('table.alignment.default', "left")

## quality of png's
dpi <- 750

## theme updates; please adjust to clientÂ´s website
#theme_set(ggthemes::theme_clean(base_size = 15))
theme_set(ggthemes::theme_clean(base_size = 15))
theme_update(plot.margin = margin(30, 30, 30, 30),
             plot.background = element_rect(color = "white",
                                            fill = "white"),
             plot.title = element_text(size = 20,
                                       face = "bold",
                                       lineheight = 1.05,
                                       hjust = .5,
                                       margin = margin(10, 0, 25, 0)),
             plot.title.position = "plot",
             plot.caption = element_text(color = "grey40",
                                         size = 9,
                                         margin = margin(20, 0, -20, 0)),
             plot.caption.position = "plot",
             axis.line.x = element_line(color = "black",
                                        size = .8),
             axis.line.y = element_line(color = "black",
                                        size = .8),
             axis.title.x = element_text(size = 16,
                                         face = "bold",
                                         margin = margin(t = 20)),
             axis.title.y = element_text(size = 16,
                                         face = "bold",
                                         margin = margin(r = 20)),
             axis.text = element_text(size = 11,
                                      color = "black",
                                      face = "bold"),
             axis.text.x = element_text(margin = margin(t = 10)),
             axis.text.y = element_text(margin = margin(r = 10)),
             axis.ticks = element_blank(),
             panel.grid.major.x = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.major.y = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.minor.x = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.minor.y = element_blank(),
             panel.spacing.x = unit(4, "lines"),
             panel.spacing.y = unit(2, "lines"),
             legend.position = "top",
             legend.title = element_text(family = "Montserrat",
                                         color = "black",
                                         size = 14,
                                         margin = margin(5, 0, 5, 0)),
             legend.text = element_text(family = "Montserrat",
                                        color = "black",
                                        size = 11,
                                        margin = margin(4.5, 4.5, 4.5, 4.5)),
             legend.background = element_rect(fill = NA,
                                              color = NA),
             legend.key = element_rect(color = NA, fill = NA),
             #legend.key.width = unit(5, "lines"),
             #legend.spacing.x = unit(.05, "pt"),
             #legend.spacing.y = unit(.55, "pt"),
             #legend.margin = margin(0, 0, 10, 0),
             strip.text = element_text(face = "bold",
                                       margin = margin(b = 10)))
## theme settings for flipped plots
theme_flip <-
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_line(size = .6,
                                          color = "#eaeaea"))
## theme settings for charts without y axis
theme_blank <- 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        axis.line.y = element_blank(),
        axis.text.y = element_blank())

## numeric format for labels
num_format <- scales::format_format(big.mark = ",", small.mark = ",", scientific = F)
## main color backlinko
bl_col <- "#00d188"
bl_dark <- darken(bl_col, .3, space = "HLS")
```


# Read data

```{r}
df <- bind_rows(
    pmap_df(list(0:99), ~read_csv(glue("../proc_data/ahref/export_keywords_100_to_500/{.x}.csv"))) %>% 
        clean_names() %>% mutate(cat = "100-500"),
    pmap_df(list(0:99), ~read_csv(glue("../proc_data/ahref/export_keywords_500_to_1000/{.x}.csv"))) %>% 
        clean_names() %>% mutate(cat = "500-1000"),
    pmap_df(list(0:49), ~read_csv(glue("../proc_data/ahref/export_keywords_1000_to_10000/{.x}.csv"))) %>% 
        clean_names() %>% mutate(cat = "1000-10000"),
    pmap_df(list(0:33), ~read_csv(glue("../proc_data/ahref/export_keywords_10000_to_1000000000/{.x}.csv"))) %>% 
            clean_names() %>% mutate(cat = "10000+")
)
```


<br>

# Preamble


<br>

The volume looks weird. It doesn't follow the initial categories:

```{r}
df %>% drop_na(volume) %>% 
    group_by(cat) %>% 
    summarise(
        n = n(),
        mean = mean(volume),
        median = median(volume),
        max = max(volume),
        min = min(volume)
        ) %>% 
    arrange(median) %>% 
    pander()
```

Let's look at some of the searches that had a low search volume in the initial data set:

```{r}
df %>% filter(cat == "100-500", volume > 10000) %>% 
    select(keyword, volume) %>% 
    head(5) %>% 
    pander()
```

This doesn't looks like low volume words. And indeed they were not in the original data sets for 100-500 volume. I'm not sure how they came in.

This removes those keywords that were not in the same category in the data samples I created:


```{r}
df_orig <- bind_rows(
    pmap_df(list(0:99), ~read_csv(glue("../raw_data/keywords_100_to_500/{.x}.txt"), col_names = c("keyword"))) %>% 
        clean_names() %>% select(keyword) %>% mutate(cat = "100-500"),
    pmap_df(list(0:99), ~read_csv(glue("../raw_data/keywords_500_to_1000/{.x}.txt"), col_names = c("keyword"))) %>% 
        clean_names() %>% mutate(cat = "500-1000"),
    pmap_df(list(0:99), ~read_csv(glue("../raw_data/keywords_1000_to_10000/{.x}.txt"), col_names = c("keyword"))) %>% 
        clean_names() %>% mutate(cat = "1000-10000"),
    pmap_df(list(0:99), ~read_csv(glue("../raw_data/keywords_10000_to_1000000000/{.x}.txt"), col_names = c("keyword"))) %>% 
            clean_names() %>% mutate(cat = "10000+")
)
```

```{r}
df %<>% inner_join(df_orig, by = c("cat", "keyword"))
```

There are still ~2.5 million rows left:

```{r}
tibble("Number of rows" = format(df %>% nrow(), big.mark = ",")) %>% 
  pander()
```




Another issue is that of the representativeness of the samples. Keywords with less than 100 volume are not even represented. And the othe rvolumes are represented at skewed ratios. This makes a large difference, especially if we look at stats based on search instead of based on volume.

To me, it still makes most sense to look at it based on volume. But you've been quite clear that this is not what we want. Still, it seems wrong to me to use this unrepresentative dataset, and report things such as mean. That will fully depend on how we happened to create the samples. Since most of searches have low volume, removing those with volume below 100 makes a huge difference.

So, for analyses where this is important, I will perform them in three different ways.

1. Using the samples here directly

2. Using the samples here, but scaled so that they are representative of the original data set. (Except that < 100 volume is removed.)

3. Using the scaled samples, and also go by volume instead of count.

As we can see, these three approaches give quite different results.

```{r}
length_keyword_files <- function(min, max){
  sql <- glue("SELECT count(*) as `count`
          FROM `dataforseo-bigquery.dataforseo_data.keyword_data` 
          WHERE location = 2840 
          AND keyword_info_search_volume >= {min}
          AND keyword_info_search_volume < {max}")
  tb <- bq_project_query("dataforseo-bigquery", sql)
  df <- bq_table_download(tb) %>% mutate(min = min, max = max)
}

scaling <- map2_df(c(0, 100, 500, 1000, 10000), c(100, 500, 1000, 10000, 1000000000), length_keyword_files) %>% 
    mutate(factor = count / 1000000) %>% relocate(min, max)

scaling %>% pander()
```


<br>


```{r}
df %<>% mutate(
  difficulty_cat = case_when(
    difficulty <= 10 ~ "Easy\n(0-10)",
    between(difficulty, 11, 30) ~ "Medium\n(11-30)",
    between(difficulty, 31, 70) ~ "Hard\n(31-70)",
    between(difficulty, 71, 100) ~ "Super hard\n(71-100)"
  )) %>% 
  mutate(difficulty_cat = factor(difficulty_cat, levels = c("Easy\n(0-10)", "Medium\n(11-30)", "Hard\n(31-70)", "Super hard\n(71-100)"))) %>% 
  mutate(log_volume = log10(volume))

dfs <- df %>% sample_n(20000) 

rdf <- bind_rows(
  df %>% filter(between(volume, 100, 500)) %>% 
    sample_n(scaling %>% filter(min == 100) %>% pull(factor) * 2000),
  df %>% filter(between(volume, 100, 500)) %>% 
    sample_n(scaling %>% filter(min == 500) %>% pull(factor) * 2000),
  df %>% filter(between(volume, 1000, 10000)) %>% 
    sample_n(scaling %>% filter(min == 1000) %>% pull(factor) * 2000),
  df %>% filter(between(volume, 10000, 1000000000)) %>% 
    sample_n(scaling %>% filter(min == 10000) %>% pull(factor) * 2000)
)
```


# Keyword difficulty

Mean and median of sample:

```{r}
tribble(~Mean, ~Median,
        round(mean(df$difficulty, na.rm = T), 2), median(df$difficulty, na.rm = T)) %>% 
  pander()
```

Mean and median of representative sample:

```{r}
tribble(~Mean, ~Median,
        round(mean(rdf$difficulty, na.rm = T), 2), median(rdf$difficulty, na.rm = T)) %>% 
  pander()
```

Mean and median of sample by volume


```{r}
tribble(~Mean, ~Median,
        round(weighted.mean(rdf$difficulty, rdf$volume, na.rm = T), 2), weighted.median(rdf$difficulty, rdf$volume)) %>% 
  pander()
```


```{r difficulty_volume_sample}
dfs %>%
  ggplot(aes(x = volume, y = difficulty)) +
  geom_jitter(size = 0.1, alpha = 0.25, height = 0.08, width = 0.3, color = "grey20") +
  scale_x_log10(labels = comma, breaks = c(1, 10, 100, 1000, 10000, 100000, 1000000)) +
  geom_smooth(method='lm', formula= y~x, color = bl_col) +
  theme(panel.grid.minor.x = element_blank()) +
  labs(title = "Popular Keywords Have Higher Keyword Difficulty Scores", 
       x = "Monthly search volume", y = "Difficulty") +
  ggsave(here::here("plots", "reworked", "volume_difficulty.pdf"),
         width = 10, height = 7, device = cairo_pdf)
```

For each doubling of volume, the difficulty increases by 1.63:

```{r}
lm1 <- lm(difficulty ~ log2(volume), dfs)
lm1 %>% summary()
```

```{r difficulty_volume_violin_sample}
dfs %>% drop_na(difficulty_cat) %>% 
  ggplot(aes(y = volume, x = difficulty_cat)) +
  geom_violin(draw_quantiles = c(0.5), color = bl_dark) +
  scale_y_log10(labels = comma) +
  labs(x = "Difficulty category", title = "Keyword difficulty and volume")
```

People will most likely not understand a violin chart. Alternatives? box blot?

!!!J: In my experience people understand violin charts equally well as box charts, and I have shown them quite a few times. It is almost a box chart, with a median, and some area above and below. So I think we can keep it. If you insist, it's an easy change to make box charts instead. Although I do think they are not really needed here, since the scatter plots are good.


```{r difficulty_cpc_scatter}
dfs %>%
  ggplot(aes(x = difficulty, y = cpc)) +
  geom_jitter(size = 0.1, alpha = 0.1, height = 0.08, width = 0.3) +
  scale_y_log10(labels = c("0.01", "0.1", "1", "10", "100"), breaks = c(0.01, 0.1, 1, 10, 100)) +
  geom_smooth(method='lm', formula= y~x) +
  labs(title = "Keyword difficulty and cpc")
```


```{r difficulty_cpc_violin}
dfs %>% drop_na(difficulty_cat) %>% 
  ggplot(aes(y = cpc, x = difficulty_cat)) +
  geom_violin(draw_quantiles = c(0.5), color = bl_dark, 
              fill = colorspace::desaturate(bl_dark, .3), 
              alpha = .4, size = .8) +
  scale_y_log10(labels = c("0.01", "0.1", "1", "10", "100"), breaks = c(0.01, 0.1, 1, 10, 100)) +
  theme(panel.grid.major.x = element_blank()) +
  labs(x = "Difficulty category", y = "Cost per click", title = "Keyword difficulty and cpc") +
  ggsave(here::here("plots", "reworked", "cpc_difficulty.pdf"),
         width = 10, height = 7, device = cairo_pdf)

dfs %>% drop_na(difficulty_cat) %>% 
  ggplot(aes(y = cpc, x = difficulty_cat)) +
  geom_violin(draw_quantiles = c(0.5), color = bl_dark, 
              fill = colorspace::desaturate(bl_dark, .3), 
              alpha = .4, size = .8, scale = "count") +
  scale_y_log10(labels = c("0.01", "0.1", "1", "10", "100"), breaks = c(0.01, 0.1, 1, 10, 100)) +
  theme(panel.grid.major.x = element_blank()) +
  labs(x = "Difficulty category", y = "Cost per click", title = "Keyword difficulty and cpc") +
  ggsave(here::here("plots", "reworked", "cpc_difficulty_true.pdf"),
         width = 10, height = 7, device = cairo_pdf)

dfs %>% drop_na(difficulty_cat) %>% 
  ggplot(aes(y = cpc, x = difficulty_cat)) +
  geom_boxplot(color = bl_dark, fill = "grey75", 
               size = 1.1, width = .7) +
  scale_y_log10(labels = c("0.01", "0.1", "1", "10", "100"), breaks = c(0.01, 0.1, 1, 10, 100)) +
  theme(panel.grid.major.x = element_blank()) +
  labs(x = "Difficulty category", y = "Cost per click", title = "Keywords With High Keyword Difficulty Scores Have Higher CPCs") +
  ggsave(here::here("plots", "reworked", "cpc_difficulty_boxplot.pdf"),
         width = 10, height = 7, device = cairo_pdf)
```





!!!D: similar comments as above. 

!!!D: Shame we cannot use the keyword categories here. May be we can try to solve this after the 3rd of November if time remains. Just curios how large the df would be if we select just the columns keyword and info_categories from the original data set. We could do on the google big query page to avoid that rstudio crashes (plus, apply some filters). Just curios to know: You probably have thought of running a left join but why is that not possible?  

!!!J: How would you do that exactly in practice? Like, what do I left join on, concretely? I know, the keywords from here. But how do I get that list into the database or the SQL command? Not saying it's not possible, Im just not sure how to do it.



<br>

# SERP features

Note there are (at least) two additional SERP feature types, knowledge panel and videos, for which the sample size is too small to be included.

```{r}
dff <- dfs %>% 
  select(keyword, volume, clicks, cpc, serp_features, cps) %>% 
  separate_rows(serp_features, sep = ",") %>% 
  mutate(serp_features = ifelse(is.na(serp_features), "(None)", serp_features)) %>% 
  filter(!(serp_features %in% c("Videos", "Knowledge panel")))

nones <- dff %>% filter(serp_features == "(None)")

dffn <- dff %>% group_by(keyword) %>% 
  summarise(n_serp = n()) %>% 
  mutate(n_serp = ifelse(keyword %in% nones$keyword, 0, n_serp)) %>% 
  mutate(n_serp = ifelse(n_serp >= 6, "6+", as.character(n_serp))) %>% 
  mutate(n_serp = factor(n_serp, levels = c("0", "1", "2", "3", "4", "5", "6+")))

dffn <- left_join(dffn, dfs, by = "keyword")
```



In sample:


```{r serp_presence_orig, fig.height = 6.5, fig.width = 10}
dff %>% group_by(serp_features) %>% 
  summarise(prop = n() / nrow(dfs)) %>% 
  ggplot(aes(y = reorder(serp_features, prop), x = prop)) +
  geom_bar(stat = "identity", fill = bl_dark, width = 0.65) +
  scale_x_continuous(labels = scales::label_percent(accuracy = 1), expand = c(0, 0)) +
  theme(panel.grid.minor.x = element_blank(), 
        panel.grid.major.y = element_blank(), 
        axis.line.y = element_blank()) +
  labs(y = "SERP feature", x = NULL, title = "Presence of SERP features")#+
  #ggsave(here::here("plots", "reworked", "serp_features.pdf"),
  #       width = 10, height = 6.5, device = cairo_pdf)
```

```{r}
rdff <- rdf %>% 
  select(keyword, volume, clicks, cpc, serp_features, cps) %>% 
  separate_rows(serp_features, sep = ",") %>% 
  mutate(serp_features = ifelse(is.na(serp_features), "(None)", serp_features)) %>% 
  filter(!(serp_features %in% c("Videos", "Knowledge panel")))

nones <- rdff %>% filter(serp_features == "(None)")

rdffn <- rdff %>% group_by(keyword) %>% 
  summarise(n_serp = n()) %>% 
  mutate(n_serp = ifelse(keyword %in% nones$keyword, 0, n_serp)) %>% 
  mutate(n_serp = ifelse(n_serp >= 6, "6+", as.character(n_serp))) %>% 
  mutate(n_serp = factor(n_serp, levels = c("0", "1", "2", "3", "4", "5", "6+")))

rdffn <- left_join(dffn %>% select(keyword, n_serp), dfs, by = "keyword")
```

Representative:

```{r serp_presence_repr, fig.height = 6.5, fig.width = 10}
rdff %>% group_by(serp_features) %>% 
  summarise(prop = n() / nrow(rdff)) %>% 
  ggplot(aes(y = reorder(serp_features, prop), x = prop)) +
  geom_bar(stat = "identity", fill = bl_dark, width = 0.65) +
  scale_x_continuous(labels = scales::label_percent(accuracy = 1), expand = c(0, 0),
                     limits = c(0, .2)) +
  theme(panel.grid.minor.x = element_blank(), 
        panel.grid.major.y = element_blank(), 
        axis.line.y = element_blank()) +
  labs(y = "SERP feature", x = NULL, title = "Presence of SERP features") +
  ggsave(here::here("plots", "reworked", "serp_features.pdf"),
         width = 10, height = 6.5, device = cairo_pdf)
```

By volume:

```{r serp_presence_volume, fig.height = 5, fig.width = 7}
rdff %>% group_by(serp_features) %>% 
  summarise(prop = sum(volume) / sum(rdff$volume)) %>% 
  ggplot(aes(y = reorder(serp_features, prop), x = prop)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black", width = 0.8) +
  scale_x_continuous(labels = scales::percent) +
  labs(y = "SERP feature", x = NULL, title = "Presence of SERP features")
```

In sample:


```{r serp_presence_n_orig}
dffn %>% group_by(n_serp) %>%
  summarise(n = n() / nrow(dffn)) %>% 
  ggplot(aes(x = n_serp, y = n)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black", width = 0.8) +
  labs(x = "Number of serp features", y = NULL, title = "Distribution of SERP features") +
  scale_y_continuous(labels = scales::percent)
```

By volume:

```{r serp_presence_n_volume}
rdffn %>% group_by(n_serp) %>%
  summarise(prop = sum(volume) / sum(rdff$volume)) %>% 
  ggplot(aes(x = n_serp, y = prop)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black", width = 0.8) +
  labs(x = "Number of serp features", y = NULL, title = "Distribution of SERP features") +
  scale_y_continuous(labels = scales::percent)
```



The knowledge card has a huge effect in reducing the cps, while the other SERP features have limited effect. Searches with the Shopping results SERP feature have higher cps on average.

```{r serp_cps, fig.width = 10, fig.height = 7.5}
order <- dff %>% 
  group_by(serp_features) %>%
  summarise(mean_cps = mean(cps, na.rm = T)) %>% 
  arrange(mean_cps) %>% 
  pull(serp_features)

  
dff %>% 
  mutate(serp_features = factor(serp_features, levels = order)) %>% 
  ggplot(aes(y = serp_features, x = cps)) +
  stat_density_ridges(fill = colorspace::desaturate(bl_col, .2), color = bl_dark, alpha = .7, size = 1) +
  scale_x_continuous(expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  labs(x = "Cost per sale (CPS)", y = "SERP feature", title = "SERP features and cost per sale") +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank()) +
  ggsave(here::here("plots", "reworked", "serp_cps.pdf"),
         width = 10, height = 7.5, device = cairo_pdf)
```

```{r serp_volume_ridge}
order <- dff %>% 
  group_by(serp_features) %>%
  summarise(mean_volume = mean(volume, na.rm = T)) %>% 
  arrange(mean_volume) %>% 
  pull(serp_features)

  
dff %>% 
  mutate(serp_features = factor(serp_features, levels = order)) %>% 
  ggplot(aes(y = serp_features, x = volume)) +
  scale_x_log10(labels = comma) +
  stat_density_ridges(fill = "turquoise4", color = "black") +
  labs(y = "SERP feature", title = "SERP features and cps")

  
dff %>% 
  mutate(serp_features = factor(serp_features, levels = order)) %>% 
  ggplot(aes(y = serp_features, x = volume)) +
  stat_density_ridges(fill = colorspace::desaturate(bl_col, .2), color = bl_dark, alpha = .7, size = 1) +
  scale_x_log10(labels = comma) +
  coord_cartesian(clip = "off") +
  labs(x = "Volume", y = "SERP feature", title = "SERP features and volume") +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank()) +
  ggsave(here::here("plots", "reworked", "serp_volume.pdf"),
         width = 10, height = 7.5, device = cairo_pdf)
```


Low difficulty keywords have fewer SERP features 

```{r, serp_difficulty_boxplot, fig.height = 6.5, fig.width = 10}
rdffn %>% mutate(n_serp = as.numeric(n_serp)) %>% 
  drop_na(difficulty_cat) %>% 
  ggplot(aes(x = difficulty_cat, y = n_serp)) +
  geom_boxplot(color = bl_dark, fill = "grey92", size = .7, width = .7) +
  labs(x = "Difficulty", y = "SERP features", title = "Difficulty and number of SERP features") +
  ggsave(here::here("plots", "reworked", "serp_difficulty_box.pdf"),
         width = 10, height = 6.5, device = cairo_pdf)
```


```{r serp_difficulty_n, fig.width = 10, fig.height = 6.5}
rdffn %>% group_by(n_serp) %>%
  summarise(difficulty = mean(difficulty, na.rm = T)) %>% 
  ggplot(aes(x = n_serp, y = difficulty)) +
  geom_bar(stat = "identity", fill = bl_dark, color = "black", width = 0.8) +
  geom_text(aes(label = format(round(difficulty, 1), scientific = FALSE)),
            nudge_y = .7, family = "Montserrat", fontface = "bold", color = "grey40") +
  labs(x = "Number of SERP features", title = "Number of SERP features and mean difficulty", y = "Difficulty") +
  coord_cartesian(clip = "off") +
  scale_y_continuous(expand = c(.001, .001)) +
  theme(panel.grid.major.x = element_blank()) +
  ggsave(here::here("plots", "reworked", "serp_mean_difficulty.pdf"),
         width = 10, height = 6.5, device = cairo_pdf)
```

```{r serp_pairs, fiog.width = 10, fig.height = 5}
find_pairs <-  function(){
  rs <- rdff %>% filter(serp_features != "(None)") %>% 
    sample_n(20000)
  kw <- rs %>% distinct(keyword) %>% pull(keyword)
  
  get_table <- function(k){
    a <- rs %>% filter(keyword == k, serp_features != "(None)")
    crossing(v1 = a$serp_features, v2= a$serp_features) %>% filter(v1 < v2)
  }
  
  pairs <- map_dfr(kw, get_table) %>% 
    group_by(v1, v2) %>% 
    summarise(n = n())
  
  pairs %>% write_csv("../proc_data/serp_pairs.csv")
}

pairs <- read_csv("../proc_data/serp_pairs.csv") %>% 
  mutate(n = n / sum(n))

pairs %>% 
  arrange(desc(n)) %>%
  head(10) %>% 
  mutate(pair = glue("{v1} & {v2}")) %>% 
  ggplot(aes(y = reorder(pair, n), x = n)) +
  geom_bar(stat = "identity", fill = bl_dark, width = 0.65) +
  labs(x = NULL, y = "SERP feature pair", title = "Most common SERP feature pairings") +
  scale_x_continuous(labels = scales::label_percent(accuracy = .1), 
                     breaks = seq(0, .15, by = .025), expand = c(0, 0), limits = c(0, .15)) +
  theme(panel.grid.minor.x = element_blank(), 
        panel.grid.major.y = element_blank(), 
        axis.line.y = element_blank()) +
  ggsave(here::here("plots", "reworked", "serp_pairings.pdf"),
         width = 10, height = 5, device = cairo_pdf)
```


Definition SERP feautres: A SERP feature is any result on a Google Search Engine Results Page (SERP) that is not a traditional organic result. The most common SERP Features are: Rich Snippets which add a visual layer to an existing result (e.g., review stars for product ratings).

!!!D: after the 3rd, may make sense to check if there is anything interesting on relationship between keyword category and SERP features. 

Some definitions:

The Clicks column shows exactly how many times per month people tend to click any pages when googling this keyword. Some searches result in a lot of clicks, while other high search volume keywords may not bring in as much traffic from search due to the low number of clicks.

The keyword "chauffeur" has a high search volume of 67,000 searches per month. Yet that volume only resulted in 13,406 clicks. One probable reason could be that Google already gave what people wanted instantly - and there was no need to click on the search results.

!!!D: CPS column definition 

The CPS (Click per Search) shows an average number of clicks for all searches. It is basically a correlation between the Clicks metric and the Search Volume of the keyword. 

In the example given, people search for "wow chauffeur" less frequently than "chauffeur", yet the keyword has more Clicks than Searches. 

Further investigation reveals that the word "wow" actually stands for "World of Warcraft", and apparently, people are looking for information on how to summon a "chauffeur" in the game. That makes for a completely different search intent. 

And this is why we have the CPS metric. 

The higher the CPS (i.e people clicking on a few links to satisfy their search query) -- the more chances that you'll get some traffic even if you're not ranking #1 for that search query. 

More info: https://help.ahrefs.com/en/articles/624151-what-does-clicks-stand-for-in-keywords-explorer 


<br>

# Global volume

```{r}
dfv <- 
  bind_rows(
    df %>% mutate(region = "US"),
    df %>% mutate(volume = global_volume - volume, region = "International")
    )
```

In *ahref*, international volume is higher:

```{r}
dfv %>% group_by(region) %>% 
  summarise(volume = sum(volume, na.rm = T)) %>% 
  mutate(volume = scales::percent(volume / sum(volume))) %>% 
  pander()
```

However, in the original data set, US volume is much higher:


```{r}
tribble(~region, ~volume,
        "US", "82%",
        "International", "19%")
```

I will be going by *ahref* in the following.

Internationally there are more searches with very low volume, while US has more searches with medium volume.

```{r comparison_count, fig.width = 10, fig.height = 6}
dfv %>% drop_na(volume) %>% 
  mutate(volume_group = case_when(volume < 100 ~ "< 100",
    between(volume, 100, 1000) ~ "100 - 1000",
    between(volume, 1000, 10000) ~ "1000 - 10,000",
    volume > 10000 ~ "10,000 +")) %>% 
  mutate(volume_group = factor(volume_group, levels = c("< 100", "100 - 1000", "1000 - 10,000", "10,000 +"))) %>% 
  group_by(volume_group, region) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(n = n / sum(n)) %>% 
  ggplot(aes(x = volume_group, y = n, fill = region)) +
  geom_bar(stat = "identity", position = position_dodge(), width = 0.8, color = "black") +
  labs(fill = "Region", x = "Volume", y = NULL) +
  scale_y_continuous(labels = scales::percent, expand = c(0,0)) +
  scale_fill_manual(values = c("grey70", bl_col), name = NULL) +
  theme(panel.grid.major.x = element_blank(), legend.position = c(.7, .8),
        legend.text = element_text(size = 14, color = "grey40", face = "bold")) +
  ggsave(here::here("plots", "reworked", "volume_region.pdf"),
         width = 10, height = 6, device = cairo_pdf)
```
There is not a large difference in the number of searches with very high volume. However, the total volume of these searches is a lot higher internationally


```{r comparison_volume, fig.width = 10, fig.height = 6}
dfv %>% drop_na(volume) %>% 
  mutate(volume_group = case_when(volume < 100 ~ "< 100",
    between(volume, 100, 1000) ~ "100 - 1000",
    between(volume, 1000, 10000) ~ "1000 - 10,000",
    between(volume, 10000, 100000) ~ "10,000 - 100,000",
    between(volume, 100000, 1000000) ~ "100,000 - 1M",
    volume > 1000000 ~ "1M +")) %>% 
  mutate(volume_group = factor(volume_group, levels = c("< 100", "100 - 1000", "1000 - 10,000", "10,000 - 100,000", "100,000 - 1M", "1M +"))) %>% 
  group_by(volume_group, region) %>% 
  summarise(n = sum(volume)) %>%
  ungroup() %>% 
  mutate(n = n / sum(n)) %>% 
  ggplot(aes(x = volume_group, y = n, fill = region)) +
  geom_bar(stat = "identity", position = position_dodge(), width = 0.8, color = "black") +
  labs(fill = "Region", x = "Volume", y = NULL) +
  scale_y_continuous(labels = scales::percent, expand = c(0,0)) +
  scale_fill_manual(values = c("grey70", bl_col), name = NULL) +
  theme(panel.grid.major.x = element_blank(), legend.position = c(.2, .5),
        legend.text = element_text(size = 14, color = "grey40", face = "bold")) +
  ggsave(here::here("plots", "reworked", "volume_region_high.pdf"),
         width = 10, height = 6, device = cairo_pdf)
```


?? CEDRIC: Percentage correct here? Was decimal numbers before but no clue why.


```{r international_v_us_volume, fig.width = 10, fig.height = 8.5}
df_int <- df %>% mutate(international_volume = global_volume - volume) %>% 
  filter(international_volume > 0, global_volume > 0) %>% 
  mutate(volume_diff = log10(international_volume) - log10(volume))

df_int_s <- df_int %>% sample_n(200000)

df_int_s %>% 
  ggplot(aes(x = volume, y = international_volume)) +
  geom_jitter(size = 0.1, alpha = 0.05, height = 0.15, width = 0.15, color = colorspace::desaturate(bl_dark, .3)) +
  geom_abline(intercept = 0, slope = 1, color = bl_col, size = 1) +
  scale_x_log10(labels = comma, expand = c(0,0)) +
  scale_y_log10(labels = comma, expand = c(0,0)) +
  labs(x = "US volume", y = "International volume") +
  ggsave(here::here("plots", "reworked", "volume_region_corr.pdf"),
         width = 10, height = 6, device = cairo_pdf)

df_int_s %>% 
  ggplot(aes(x = volume, y = international_volume)) +
  geom_hex(color = "white") +
  geom_abline(intercept = 0, slope = 1, color = bl_col, size = 1) +
  scale_x_log10(labels = comma, expand = c(.03, .03)) +
  scale_y_log10(labels = comma, expand = c(.03, .03)) +
  scale_fill_gradient(low = "grey85", high = bl_dark, 
                      name = "Number of observations",
                      breaks = seq(500, 9500, by = 1500)) +
  guides(fill = guide_colorbar(title.position = "top", 
                               title.hjust = .5,
                               direction = "horizontal",
                               barwidth = unit(16, "lines"),
                               barheight = unit(.4, "lines"))) +
  labs(x = "US volume", y = "International volume") +
  theme(legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        legend.position = c(.75, .1)) +
  ggsave(here::here("plots", "reworked", "volume_region_corr_hex.pdf"),
         width = 10, height = 8.5, device = cairo_pdf)
```

<br>

We can see that they mostly follow each other, but there are some searches with large difference between them.

Higher volume internationally:

```{r}
tbl <- df_int %>% 
  arrange(desc(volume_diff)) %>% 
  filter(volume != 60) %>% 
  select(keyword, us_volume = volume, international_volume) %>% 
  head(5)

tbl %>% write_csv("../plots/csv/table_int.csv")
tbl %>% pander()
```

<br>

Higher volume in US:

```{r}
tbl <- df_int %>% 
  arrange(volume_diff) %>% 
  select(keyword, us_volume = volume, international_volume) %>% 
  head(5)

tbl %>% write_csv("../plots/csv/table_us.csv")
tbl %>% pander()
```

<br>


Searches that have higher volume in US have a higher click-per-search on average than searches that have higher volume internationally.

```{r international_cps}
df_int_s %>% 
  filter(cps < 5) %>% 
  ggplot(aes(x = volume_diff, y = cps)) +
  geom_point(size = 0.3, alpha = 0.05, color = "grey20") +
  geom_smooth(method='lm', formula= y~x, color = bl_col, size = 2) +
  scale_x_continuous(breaks = c(-2, 0, 3), labels = c("More US", "0", "More international")) +
  labs(x = NULL, y = "Cost per sale (CPS)") +
  ggsave(here::here("plots", "reworked", "volume_region_cps_corr.pdf"),
         width = 10, height = 7, device = cairo_pdf)
```
<br>

They also have a higher cost-per-click on average

```{r international_cpc, fig.width = 10, fig.height = 7}
df_int_s %>% 
  ggplot(aes(x = volume_diff, y = cpc)) +
  geom_point(size = 0.3, alpha = 0.05, color = "grey20") +
  geom_smooth(method='lm', formula= y~x, color = bl_col, size = 2) +
  scale_x_continuous(breaks = c(-2, 0, 3), labels = c("More US", "0", "More international")) +
  scale_y_log10(labels = comma) +
  labs(x = NULL, y = "CPC") +
  ggsave(here::here("plots", "reworked", "volume_region_cpc_corr.pdf"),
         width = 10, height = 7, device = cairo_pdf)
```
<br>

Searches that have higher volume internationally, tend to have higher difficulty

```{r international_difficulty, fig.width = 10, fig.height = 7}
df_int_s %>% 
  ggplot(aes(x = volume_diff, y = difficulty)) +
  geom_point(size = 0.3, alpha = 0.05, color = "grey20") +
  geom_smooth(method='lm', formula= y~x, color = bl_col, size = 2) +
  scale_x_continuous(breaks = c(-2, 0, 3), labels = c("More US", "0", "More international")) +
  labs(x = NULL, y = "Difficulty") +
  ggsave(here::here("plots", "reworked", "volume_region_difficulty_corr.pdf"),
         width = 10, height = 7, device = cairo_pdf)
```

<br>

# Clicks

In sample:

```{r}
tribble(~Mean, ~Median,
        round(mean(df$clicks, na.rm = T), 2), median(df$clicks, na.rm = T)) %>% 
  pander()
```

In representative sample:

```{r}
tribble(~Mean, ~Median,
        round(mean(rdf$clicks, na.rm = T), 2), median(rdf$clicks, na.rm = T)) %>% 
  pander()
```

By volume

```{r}
tribble(~Mean, ~Median,
        round(weighted.mean(rdf$clicks, rdf$volume, na.rm = T), 2), weighted.median(rdf$clicks, rdf$volume, na.rm = T)) %>% 
  pander()
```


```{r dist_clicks_sample}
log_mean <- 10 ^ (df %>% mutate(clicks = clicks + 1) %>% 
  mutate(log_clicks = log10(clicks)) %>% 
  summarise(m = mean(log_clicks, na.rm = T)) %>% 
  pull(m))

df %>% ggplot(aes(x = clicks)) +
  geom_histogram(fill = "turquoise4", color = "black") +
  scale_x_log10(labels = comma) +
  scale_y_continuous(limits = c(0, 250000), expand = c(0,0)) +
  labs(title = "Distribution of number of clicks", y = NULL, x = NULL) +
  geom_vline(xintercept = log_mean, linetype = "dashed", color = "blue", size = 1) +
  ggeasy::easy_remove_y_axis()
```
Note that this is in sample, so the lowest part of the distribution is not included. Probably does not really make sense.

<br>



# Return Rate

Comparison of searches with same volume but different return rates:

```{r}
tbl <- bind_rows(
  df %>% filter(return_rate > 10) %>% 
    summarise(mean_cpc = mean(cpc, na.rm = T), mean_clicks = mean(clicks, na.rm = T), mean_cpc = mean(cps, na.rm = T), mean_difficulty = mean(difficulty, na.rm = T)) %>% 
    mutate(return_rate = "very high") %>% relocate(return_rate),

  df %>% filter(return_rate > 10) %>% 
    select(number, volume) %>% 
    left_join(df %>% filter(return_rate < 10), by = c("number", "volume")) %>% 
    distinct(number, volume, cat, .keep_all = T) %>% 
    summarise(mean_cpc = mean(cpc, na.rm = T), mean_clicks = mean(clicks, na.rm = T), mean_cpc = mean(cps, na.rm = T), mean_difficulty = mean(difficulty, na.rm = T)) %>% 
    mutate(return_rate = "low") %>% relocate(return_rate)
)

tbl %<>% mutate(mean_cpc = round(mean_cpc, 2), mean_clicks = round(mean_clicks, 0), mean_difficulty = round(mean_difficulty, 1))
               
tbl %>% write_csv("../plots/csv/return_rate.csv")
tbl %>% pander()
```

We can see that searches with high return rates tend to have lower difficulty, and to be clicked on a lot more.

```{r convert_to_pngs, results='asis', echo=FALSE, eval=FALSE, include=FALSE}
dir_plots <- here::here("plots", "reworked")

pdfs <- list.files(dir_plots, pattern = ".*pdf", recursive = TRUE)
for(pdf in pdfs) {
  pdf_convert(pdf = glue::glue("{dir_plots}/{pdf}"), 
              filenames = glue::glue("{dir_plots}/{str_remove(pdf, '.pdf')}.png"),
              format = "png", dpi = dpi)
}
```

***

<details><summary>Session Info</summary>

```{r sessionInfo}
Sys.time()
git2r::repository()
sessionInfo()
```

</details>
