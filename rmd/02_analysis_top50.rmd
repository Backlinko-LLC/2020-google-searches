---
title: "Top50"
author: ""
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    theme: paper
    highlight: kate
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
editor_options: 
  chunk_output_type: inline
---


<style>
.list-group-item.active, .list-group-item.active:hover, .list-group-item.active:focus {
  background-color: #00d188;
  border-color: #00d188;
}

body {
  font-family: montserrat;
  color: #444444;
  font-size: 14px;
}

h1 {
  font-weight: bold;
  font-size: 28px;
}

h1.title {
  font-size: 30px;
  color: #00d188;
}

h2 {
  font-size: 24px;
}

h3 {
  font-size: 18px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F, fig.path = "../plots/1611/", cache = F, fig.showtext = TRUE, dpi = 700)
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",", small.mark = ",", scientific = F)
})
Sys.setlocale("LC_TIME", "C")
extrafont::loadfonts(device = "win")
```



```{r prep}
set.seed(1)
required_packages <- c("tidyverse", "magrittr", "DBI", "bigrquery", "arrow","glue", "vroom","janitor", "gt", "ggwordcloud", "readxl", "ggthemes", "hrbrthemes", "extrafont", "plotly", "scales", "stringr", "gganimate", "here", "tidytext", "sentimentr", "scales", "DT", "here", "sm", "mblm", "glue", "fs", "knitr", "rmdformats", "janitor", "urltools", "colorspace", "pdftools", "showtext", "pander", "wordcloud2", "stopwords", "magicfor", "gapminder", "arrow")
for(i in required_packages) { 
  if(!require(i, character.only = T)) {
    #  if package is not existing, install then load the package
    install.packages(i, dependencies = T)
  require(i, character.only = T)
  }
}
panderOptions('table.alignment.default', "left")
## quality of png's
dpi <- 750
## theme updates; please adjust to clientÂ´s website
#theme_set(ggthemes::theme_clean(base_size = 15))
theme_set(ggthemes::theme_clean(base_size = 15))
theme_update(plot.margin = margin(30, 30, 30, 30),
             plot.background = element_rect(color = "white",
                                            fill = "white"),
             plot.title = element_text(size = 20,
                                       face = "bold",
                                       lineheight = 1.05,
                                       hjust = .5,
                                       margin = margin(10, 0, 25, 0)),
             plot.title.position = "plot",
             plot.caption = element_text(color = "grey40",
                                         size = 9,
                                         margin = margin(20, 0, -20, 0)),
             plot.caption.position = "plot",
             axis.line.x = element_line(color = "black",
                                        size = .8),
             axis.line.y = element_line(color = "black",
                                        size = .8),
             axis.title.x = element_text(size = 16,
                                         face = "bold",
                                         margin = margin(t = 20)),
             axis.title.y = element_text(size = 16,
                                         face = "bold",
                                         margin = margin(r = 20)),
             axis.text = element_text(size = 11,
                                      color = "black",
                                      face = "bold"),
             axis.text.x = element_text(margin = margin(t = 10)),
             axis.text.y = element_text(margin = margin(r = 10)),
             axis.ticks = element_blank(),
             panel.grid.major.x = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.major.y = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.minor.x = element_line(size = .6,
                                               color = "#eaeaea",
                                               linetype = "solid"),
             panel.grid.minor.y = element_blank(),
             panel.spacing.x = unit(4, "lines"),
             panel.spacing.y = unit(2, "lines"),
             legend.position = "top",
             legend.title = element_text(,
                                         color = "black",
                                         size = 14,
                                         margin = margin(5, 0, 5, 0)),
             legend.text = element_text(
                                        color = "black",
                                        size = 11,
                                        margin = margin(4.5, 4.5, 4.5, 4.5)),
             legend.background = element_rect(fill = NA,
                                              color = NA),
             legend.key = element_rect(color = NA, fill = NA),
             #legend.key.width = unit(5, "lines"),
             #legend.spacing.x = unit(.05, "pt"),
             #legend.spacing.y = unit(.55, "pt"),
             #legend.margin = margin(0, 0, 10, 0),
             strip.text = element_text(face = "bold",
                                       margin = margin(b = 10)))
## theme settings for flipped plots
theme_flip <-
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_line(size = .6,
                                          color = "#eaeaea"))
## theme settings for charts without y axis
theme_blank <- 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        axis.line.y = element_blank(),
        axis.text.y = element_blank())

## numeric format for labels
num_format <- scales::format_format(big.mark = ",", small.mark = ",", scientific = F)
## main color backlinko
bl_col <- "#00d188"
bl_dark <- darken(bl_col, .3, space = "HLS")
```

I read initial *ahref* and top50 *ahref*

(I remove the 47th and 50th entry as they are both misspellings of youtube. They have low traffic compared to the others, so no real importance.)



```{r}
ahref <- read_feather("../proc_data/ahref/ahref.f")

ahref %>% filter(cat == "10000+", volume < 10000) %>% 
    drop_na(difficulty) %>% 
    arrange(volume)
```



```{r}
inner_join(
    ahref %>% group_by(cat) %>% 
    summarise(n_ahref = n() / 1000, volume_ahref = sum(volume, na.rm = T) / 1000000),
    ahref %>% group_by(cat) %>% 
        summarise(n_ahref_new = n() / 1000, volume_ahref_new = sum(volume, na.rm = T) / 1000000),
    by = "cat")
```



```{r}
top50 <- vroom::vroom("../proc_data/top_50_with_ahrefs.csv") %>% 
    janitor::clean_names() %>% 
    filter(!(keyword %in% c("the youtube", "youtubecom"))) %>% 
    mutate(last_letter = str_sub(volume, -1)) %>% 
    mutate(volume = case_when(
        last_letter == "M" ~ as.numeric(str_sub(volume, 0, -2)) * 1000000,
        last_letter == "K" ~ as.numeric(str_sub(volume, 0, -2)) * 1000
    ))
```

conform that they roughly agree




```{r, render = pander}
ahref %>% 
    rename(volume_initial = volume) %>% 
    inner_join(top50, by = "keyword") %>% 
    select(keyword, volume_initial, volume)
```

Get the volume and count for the original dataset for each group

```{r, render = pander}
length_keyword_files <- function(min, max){
  sql <- glue("SELECT SUM(COALESCE(keyword_info_search_volume / 10000, 0)) AS volume
          FROM `dataforseo-bigquery.dataforseo_data.keyword_data` 
          WHERE location = 2840 
          AND spell = ''
          AND keyword_info_search_volume >= {min}
          AND keyword_info_search_volume < {max}")
  tb <- bq_project_query("dataforseo-bigquery", sql)
  df <- bq_table_download(tb) %>% mutate(min = min, max = max, volume = 10000 * volume)
}

volume <- map2_df(c(0, 100, 500, 1000, 10000), c(100, 500, 1000, 10000, 1000000000), length_keyword_files) %>% 
    mutate(cat = case_when(
        min == 0 ~ "0-100",
        min == 100 ~ "100-500",
        min == 500 ~ "500-1000",
        min == 1000 ~ "1000-10000",
        min == 10000 ~ "10000+")) %>% 
    select(cat, volume_orig = volume)

length_keyword_files <- function(min, max){
  sql <- glue("SELECT COUNT(*) AS count
          FROM `dataforseo-bigquery.dataforseo_data.keyword_data` 
          WHERE location = 2840 
          AND spell = ''
          AND keyword_info_search_volume >= {min}
          AND keyword_info_search_volume < {max}")
  tb <- bq_project_query("dataforseo-bigquery", sql)
  df <- bq_table_download(tb) %>% mutate(min = min, max = max)
}

count <- map2_df(c(0, 100, 500, 1000, 10000), c(100, 500, 1000, 10000, 1000000000), length_keyword_files) %>% 
    mutate(cat = case_when(
        min == 0 ~ "0-100",
        min == 100 ~ "100-500",
        min == 500 ~ "500-1000",
        min == 1000 ~ "1000-10000",
        min == 10000 ~ "10000+")) %>% 
    select(cat, count_orig = count)

n_orig <- inner_join(volume, count, by = "cat") %>% 
    rename(n_orig = count_orig)
n_orig %>% 
    mutate(n_prop = n_orig / sum(n_orig),
           vol_prop = volume_orig / sum(volume_orig))
```

```{r}
n_orig_prop <- n_orig %>% 
    mutate(volume_prop = volume_orig / sum(volume_orig),
           n_prop = n_orig / sum(n_orig))
```


Overview of volume and count in the two datasets

```{r, render = pander}
ns <- ahref %>% group_by(cat) %>% 
    summarise(n_ahref = n(), volume_ahref = sum(volume, na.rm = T)) %>% 
    full_join(n_orig, by = "cat")
ns
```


Since the top50 is very important, let's split up the 10000+ category further.

```{r}
#ahref_top50_vol <- top50 %$% sum(volume)
top50_in_ahref <- ahref %>% filter(keyword %in% top50$keyword)
top50_in_ahref_vol <- top50_in_ahref %$% sum(volume)

top100k <- vroom::vroom("../proc_data/samples/top100k.csv")
orig_top50_vol <- top100k %>% filter(spell %in% top50$keyword) %>% 
    summarise(s = sum(keyword_info_search_volume)) %>% pull(s)


ns %<>% add_row(
    cat = "top 50", 
    n_ahref = top50_in_ahref %>% nrow(), 
    volume_ahref = top50_in_ahref_vol, 
    volume_orig = orig_top50_vol, 
    n_orig = 48) %>% 
    mutate(
        volume_orig = ifelse(cat == "10000+", volume_orig - orig_top50_vol, volume_orig),
        volume_ahref = ifelse(cat == "10000+", volume_ahref - top50_in_ahref_vol, volume_ahref)
        )
```

Looking at the number included in each category, compared to the total number of searches represented in the original dataset.

```{r, render = pander}
ns %<>% mutate(prop_included = n_ahref / n_orig)
ns %>% select(cat, n_ahref, n_orig, prop_included)
```

```{r, render = pander}
ns %<>% mutate(prop_included = n_ahref / n_orig)
ns %>% select(cat, n_ahref, n_orig, prop_included)
```

From this we can find the estimated volume, based on the *ahref* volume, as it is expected to have been if all keywords had been analyzed with *ahref*:


```{r, render = pander}
ns %<>% mutate(volume_ahref_scaled = volume_ahref / prop_included)
ns %>% select(cat, volume_ahref, volume_orig, prop_included, volume_ahref_scaled)
```

The top50 category is highly dependent on which 16 happened to be selected. (The others are large enough that this randomness is not a factor.) The below table lists the true top50 volume.

```{r, render = pander}
ns %<>% mutate(volume_ahref_true = ifelse(cat == "top 50", sum(top50$volume), NA))
ns %>% select(cat, volume_orig, volume_ahref_scaled, volume_ahref_true)
```

We can see that we miss some of the big ones in the top50, and the real number is ~3x.

Still, this number is only ~1.2%(!) of the number using the original data set:

```{r, render = pander}
1288773300 / 104274763000
```

Are spelling mistakes really that common that the top100 sites get 100x searches that are spelling mistakes? Seems unlikely to me. Spelling mistakes also get more low volume in *ahref* except for some misspellings that seem more obvious (eg "youtubecom"), whereas in the original data set, many misspellings have extremely high volume.

So I think the correct approach is to make an estimate of the volume based on *ahref*. This can be estimated by upscaling each category as if it was representative.

The category 100-500 has roughly the same volume in the original data set and in scaled ahref, which I think is a very good indicator for this approach being a correct approximation. 100-500 is likely the category that is least skewed by high-volume spelling mistakes.

To repeat: The fact that the 100-500 category gives the correct approximation, shows that the approximation method is correct. All the above categories give an increasingly higher volume in the original data set, but this is because the misspelled searches have too high volume, which is especially important for the high volume categories. Thus we can instead use the approximation from *ahref* which we know is accurate based on the 100-500 category.

So I will calculate the total volume as follows:

For 0-100: Use the total volume in the original data set.
For 100-500, 500-1000, 1000-10000 and 10000+: Use the upscaled volume of *ahref*
For top 50: Use the *ahref* top 50.


```{r, render = pander}
ns %<>% mutate(real_volume = case_when(
    cat == "0-100" ~ volume_orig,
    cat %in% c("100-500", "500-1000", "1000-10000", "10000+") ~ volume_ahref_scaled,
    cat == "top 50" ~sum(top50$volume)))
ns
```

This gives a total value of (in billions)

```{r, render = pander}
sum(ns$real_volume) / 1000000000
```

This is about a tenth of the volume found in the original data set (300 billion), due to the inflation from the misspelled searches. Importantly, note that we cannot simple remove all the ones with a spell_type, since a large portion (near half, it seems) of spell types are unrecognized. Thus, the total volume in the original data set, with identifiable spell types removed, still gives around 150 billion, which is still much higher.

I think the best approach is to trust *ahref* over the original data set, and use this total violume of 40 billion to draw the graphs etc that depend on total volume.


Let's start with listing the top 50 searches and their proportions:



```{r}
top50 %>% select(keyword, volume) %>% 
    bind_rows(ahref %>% select(keyword, volume)) %>% 
    arrange(desc(volume)) %>% 
    mutate(volume = volume / sum(ns$real_volume)) %>% 
    distinct(keyword, .keep_all = T) %>% 
    head(1000) %>% 
    write_csv("../plots/csv/top1k_table.csv")
top50 <- read_csv("../plots/csv/top1k_table.csv") %>% head(50)
top100 <- read_csv("../plots/csv/top1k_table.csv") %>% head(100)
```




Long tails:

```{r volume_by_searches}
top100 %>% add_rownames() %>% 
    mutate(rowname = as.numeric(rowname)) %>% 
    ggplot(aes(x = rowname, y = volume)) +
    geom_area(alpha = 0.8, fill = bl_dark) +
    labs(x = "Rank", title = "Volume of searches", y = "Volume") +
    scale_y_continuous(labels = c("0%", "0.1%", "0.2%", "0.3%", "0.4%", "0.5%", "0.6%"), limits = c(0, 0.006), expand = c(0,0)) +
    scale_x_continuous(limits = c(1, 100), expand = c(0,0))
```


<br>

In addition, all the previous results that go by volume, could be remade using 40 billion instead of 300 billion.


> We need specific numbers on this. Top 500=X% of searches. Top 2000=X% of searches. Top 10,000=X% of searches. 

This is not straightforward to find out based on the above analysis.

The best way is to take the top results from AHREF, including the top 50 volume, and those included from 10.000+. For top 50 we have the full estimated volume, but for the remaining with 10.000+ we have a representation of 23%. While that is quite good, it is not 100%. This means that we will miss some words that have higher volume than those included. Thus, this will be an underestimation of the true proportions.

One way to get a better estimate than this, the only way is to run ahref analysis on top x searches. Where x is maybe 2x the number we are interested in, to accomodate spell_types whose true volume is much lower.

A different approach, and the one I will perform here, is to upsample what we have. I take the words we have in the category, bootstrap sample until we have the true amount of searches, and then take the top list. This will not be 100% accurate, but likely be a very good approximation. Especially given that we have the true volume of the top 50 already.




```{r}
scale_factor <- 1 / ns %>% filter(cat == "10000+") %>% pull(prop_included)
ahref_10kplus <- ahref %>% anti_join(top50, by = "keyword") %>% 
    filter(cat == "10000+") %>% 
    drop_na(volume)# %>% 

ahref_10kplus_scaled <- sample_n(ahref_10kplus, scale_factor * nrow(ahref_10kplus), 
                                 replace = T) %>% 
    arrange(desc(volume))
```


```{r}
sum(top50$volume)
top10k <- ahref %>% anti_join(top50, by = "keyword") %>% 
    arrange(desc(volume)) %>% 
    drop_na(volume)


tribble(~top, ~volume,
        "Top 50", sum(top50$volume),
        "Top 500", (ahref_10kplus_scaled %>% head(450) %$% sum(volume)) / sum(ns$real_volume) +  sum(top50$volume),
        "Top 2000", (ahref_10kplus_scaled %>% head(1950) %$% sum(volume)/ 
            sum(ns$real_volume) +  sum(top50$volume)) ,
        "Top 10k", (ahref_10kplus_scaled %>% head(9950) %$% sum(volume)) / sum(ns$real_volume) +  sum(top50$volume)
        ) %>% 
    write_csv("../plots/csv/top_volume.csv")
```


```{r}
min = 0
max = 50
sql <- glue("SELECT SUM(COALESCE(keyword_info_search_volume / 10000, 0)) AS volume, COUNT(*) as count
      FROM `dataforseo-bigquery.dataforseo_data.keyword_data` 
      WHERE location = 2840 
      AND spell = ''
      AND keyword_info_search_volume >= {min}
      AND keyword_info_search_volume < {max}")
tb <- bq_project_query("dataforseo-bigquery", sql)
df <- bq_table_download(tb) %>% mutate(min = min, max = max, volume = 10000 * volume)
df$volume / sum(ns$real_volume)
df$count / sum(ns$n_orig)
```


```{r}
tibble(volume = sum(ns$real_volume),
       count = 306000000) %>% 
    write_csv("../proc_data/overview.csv")
```


```{r}
ns %>% mutate(
    vol_prop = volume_orig / sum(volume_orig),
    n_prop = n_orig / sum(n_orig))
```


```{r search_tails}
get_count_range <-  function(lower, higher)
{
  sql <- glue(
        "SELECT COUNT(*) AS `count`,  
         FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
         WHERE location = 2840 
         AND spell = ''
         AND keyword_info_search_volume > {lower} 
         AND keyword_info_search_volume <= {higher}")
      tb <- bq_project_query("dataforseo-bigquery", sql)
      bq_table_download(tb)$count
}
df <- tribble(
  ~cat, ~count,
  "0 - 10", get_count_range(0, 10),
  "11- 100", get_count_range(11, 100),
  "101 - 1000", get_count_range(101, 1000),
  "1001 - 10000", get_count_range(1001, 10000),
  "10001 - 100000", get_count_range(10001, "100000"),
  "100001+", get_count_range("100001", "100000000000")) 

n_orig_prop %<>% 
    mutate(ncat = case_when(
        cat == "0-100" ~ "1-100",
        cat == "100-500" ~ "100-1000",
        cat == "500-1000" ~ "100-1000",
        cat == "1000-10000" ~ "1000-10000",
        cat == "10000+" ~ "10000+",
    )) %>% 
    group_by(ncat) %>% 
    summarise(volume_prop = sum(volume_prop),
              n_prop = sum(n_prop)) %>% 
    mutate(cat = factor(ncat, levels = c("1-100", "100-1000", "1000-10000", "10000+")))
```

```{r proportion_search_volume}
n_orig_prop %>% 
  ggplot(aes(x = cat, y = volume_prop)) +
  geom_bar(stat = "identity", fill = bl_dark, color = "black", width = .85) +
  geom_text(aes(label = glue::glue("{format(round(volume_prop, 3) * 100, scientific = FALSE)}%")),
            nudge_y = .04, family = "Montserrat", fontface = "bold", color = "grey40", size = 3) +
  theme(panel.grid.major.x = element_blank()) +
  labs(x = "Search volume category", y = "Percentage of all search volume", title = "Volume of searches") +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1), expand = c(0.001, 0.001))
```



```{r proportion_searches}
n_orig_prop %>% 
  ggplot(aes(x = cat, y = n_prop)) +
  geom_bar(stat = "identity", fill = bl_dark, color = "black", width = .85) +
  geom_text(aes(label = glue::glue("{format(round(n_prop, 3) * 100, scientific = FALSE)}%")),
            nudge_y = .04, family = "Montserrat", fontface = "bold", color = "grey40", size = 3) +
  theme(panel.grid.major.x = element_blank()) +
  labs(x = "Search volume category", y = "Percentage of all searches", title = "Number of searches") +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1), expand = c(0.001, 0.001))
```


