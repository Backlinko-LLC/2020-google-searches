---
title: "Keyword analysis"
output:
  html_document:
    df_print: paged
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.path = "plots/")
```


## Load data

```{r}
library(pacman)
p_load(dplyr, magrittr, tidyverse, DBI, bigrquery, arrow, pander, vroom, glue, janitor, gt, ggwordcloud)
set.seed(1)
#pq_auth() in console
```



<br>

```{r}
con <- dbConnect(
    bigrquery::bigquery(),
    project = "dataforseo-bigquery",
    billing = "dataforseo-bigquery"
)

sql <- glue("SELECT * FROM `dataforseo-bigquery.dataforseo_data.keyword_data` 
          WHERE location = 2840 
          ORDER BY keyword_info_search_volume DESC
          LIMIT 50000")
tb <- bq_project_query("dataforseo-bigquery", sql)
top <- bq_table_download(tb, max_results = 1000000)
```

<br>

## Basic stats

#### Total number of different searches

```{sql, connection = con, output.var = "sql"}
SELECT COUNT(keyword_info_search_volume) as `total_count`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
```

```{r}
total_count <- sql$total_count
total_count
```
<br>
<br>


#### Total number of searches

(Calculated in a roundabout way to avoid integer overflow)

```{sql, connection = con, output.var = "sql"}
SELECT SUM(COALESCE(keyword_info_search_volume / 10000, 0)) AS total_volume
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
```

```{r}
total_volume <- sql$total_volume * 10000
total_volume
```


<br>

#### Top searches

This table shows the top 30 searches. They are all spelling errors. As in, they are not really searched, but rather people attempting to go to Youtube or Facebook, but typing it wrong. Oddly they are all attributed as having a search volume of exactly 185 million. Note that the ones that have spell_type NA are also failed attempts to go on Youtube / Facebook, just unrecognized

```{r}
top %>% 
  select(keyword, location, spell, spell_type, keyword_info_search_volume) %>% head(30)
```

<br>

#### Missing keyword_info_search_volume

```{sql, connection = con, output.var = "sql"}
SELECT COUNT(*) as `missing_count`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
AND keyword_info_search_volume IS NULL
```

```{r}
missing_count <- sql$missing_count
scales::percent(missing_count / total_count, accuracy = 0.001)
```

<br>

#### Search volume 0

```{sql, connection = con, output.var = "sql"}
SELECT COUNT(*) as `zero_count`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
AND keyword_info_search_volume = 0
```

```{r}
zero_count <- sql$zero_count
scales::percent(zero_count / total_count, accuracy = 0.001)
```

<br>

The missing have some searches that are likely high volume. Thus they are truly missing, and not just 0s.

```{sql, connection = con, output.var = "sql"}
SELECT keyword, keyword_info_search_volume
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
AND keyword_info_search_volume IS NULL
ORDER BY RAND()
LIMIT 20
```

```{r}
sql
```


<br>

#### Mean search volume


```{sql, connection = con, output.var = "sql"}
SELECT AVG(`keyword_info_search_volume`) AS `mean`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
```

```{r}
sql$mean
```

<br>

Confirm that we get the same from total_volume and total_count

```{r}
total_volume / total_count
```

<br>

#### Median search volume

```{sql, connection = con, output.var = "sql"}
SELECT approx_quantiles(keyword_info_search_volume, 2)[offset(1)] AS `median`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
```


```{r}
sql$median
```

<br>

#### Mean CPC


```{sql, connection = con, output.var = "sql"}
SELECT AVG(`keyword_info_cpc`) AS `mean_cpc`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE `location` = 2840
```


```{r}
sql$mean_cpc
```

<br>

#### Median CPC

```{sql, connection = con, output.var = "sql"}
SELECT approx_quantiles(keyword_info_cpc, 2)[offset(1)] AS `median_cpc`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
```


```{r}
sql$median_cpc
```

<br>

## Spell types

<br>

Only the top results have spell types:

```{sql, connection = con, output.var = "sql"}
SELECT *
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
AND spell_type <> ""
AND keyword_info_search_volume < 10000
LIMIT 1000
```

```{sql, connection = con, output.var = "sql"}
SELECT spell_type, SUM(keyword_info_search_volume) / 10000 AS `volume`
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
GROUP BY spell_type
```

```{r spell_types}
spell_types <- sql %>% mutate(spell_type = ifelse(spell_type == "", "no_spell_type", spell_type)) %>% 
  mutate(volume = volume * 10000) 


spell_types %>% ggplot(aes(x = reorder(spell_type, volume), y = volume)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black", fig.wiodth = 0.8) +
  theme_light() +
  labs(x = "", title = "Spell types")
```

<br>

About half of search volume has a spell type. This is especially driven by misspellings of common domains.

```{r}
spell_types %>% filter(spell_type != "no_spell_type") %$% 
  sum(volume) / total_volume
```


<br>

Top 15 intended searches that are misspelled

```{r}
top %>% group_by(spell) %>% 
  summarise(volume = sum(keyword_info_search_volume)) %>% 
  arrange(desc(volume)) %>% 
  filter(spell != "") %>% 
  head(15)
```


<br>

## Questions

```{r}
word <- "who"
sql <- glue("SELECT keyword FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
            WHERE location = 2840 
            AND keyword like '{word} %'
            LIMIT 1000")
tb <- bq_project_query("dataforseo-bigquery", sql)
  #df <- bq_table_download(tb, max_results = as.numeric(n))
  #df
df <- bq_table_download(tb) 
```


```{r questions}
question_words <- c("what", "which", "where", "who", "why", "how")
questions <- tribble(~question, ~volume)
for (word in question_words){
  sql <- glue("SELECT sum(keyword_info_search_volume) FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
            WHERE location = 2840 
            AND keyword like '{word} %' ")
  tb <- bq_project_query("dataforseo-bigquery", sql)
  df <- bq_table_download(tb) 
  questions %<>% add_row(question = word, volume = df$f0_)
}

questions %>% mutate(prop = volume / total_volume) %>% 
  ggplot(aes(x = reorder(question, prop), y = prop)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black") +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.005), expand = c(0,0)) +
  theme_light() + 
  labs(title = "Questions in searches", x = "", y = "")
```

<br>

Total percentage of questions:

```{r}
scales::percent(questions %$% sum(volume) / total_volume, accuracy = 0.001)
```

<br>

Total percentage of questions if spell types are removed:


```{sql, connection = con, output.var = "sql"}
SELECT SUM(COALESCE(keyword_info_search_volume / 10000, 0)) AS total_volume
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
AND spell_type = ""
```

```{r}
total_volume_nospell <- sql$total_volume * 10000
total_volume_nospell
```

```{r}
scales::percent(questions %$% sum(volume) / total_volume_nospell, accuracy = 0.001)
```

<br>

## Stopwords


```{r stopwords, fig.height = 8, fig.width = 6}
stopwords_list <- tibble(stopword = stopwords::stopwords(language = "en")) %>% 
  mutate(stopword = str_remove(stopword, "'")) %>% 
  filter(!(stopword %in% c("shed", "wed", "ill", "hell", "shell")))

get_stopwords_counts <- function(){
stopwords <- tribble(~stopword, ~volume)
  for (word in stopword_list$stopword){
    print(word)
    sql <- glue(
      "SELECT SUM(COALESCE(keyword_info_search_volume / 1000, 0)) AS `total_volume` 
       FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
       WHERE location = 2840 
       AND keyword like '% {word} %' OR keyword like '{word} %' OR keyword like '% {word}'")
    tb <- bq_project_query("dataforseo-bigquery", sql)
  
    df <- bq_table_download(tb) 
    stopwords %<>% add_row(stopword = word, volume = df$total_volume * 1000)
  }
  
  write_csv(stopwords, "../proc_data/stopwords.csv")
}
stopwords <- read_csv("../proc_data/stopwords.csv")

stopwords %>% mutate(prop = volume / total_volume) %>%
  filter(prop > 0.001) %>% 
  ggplot(aes(x = reorder(stopword, prop), y = prop)) +
  geom_bar(stat = "identity", color = "black", fill = "turquoise4", width = 0.7) +
  theme_light() +
  scale_y_continuous(labels = scales::percent, expand = c(0,0), limits = c(0, 0.05)) +
  labs(x = "", y = "", title = "Proportion of searches with specific stopwords") +
  coord_flip()
```
<br>

```{r stopwords_wordcloud}
set.seed(1)
stopwords %>% 
  mutate(stopword = ifelse(stopword == "i", "I", stopword)) %>% 
  arrange(desc(volume)) %>% 
  head(25) %>% 
  ggplot(aes(label = stopword, size = volume, color = factor(sample.int(10, 25, replace = TRUE)))) +
  geom_text_wordcloud(perc_step = 0.3) +
  scale_size_area(max_size = 30) +
  theme_minimal()
```




<br>

## Long tail

```{r search_tails}
volume_top <- top %>%  
  add_rownames() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  select(rowname, volume = keyword_info_search_volume)

ylab <- c(50, 100, 150, 200)

volume_top %>% 
  mutate(cat = case_when(
    rowname < 500 ~ "Top 500",
    rowname < 1000 ~ "Top 1000",
    rowname < 2000 ~ "Top 2000",
    rowname < 5801 ~ "Top 0.001%",
    T ~ "Remaining 99.99%"
  )) %>% 
  mutate(cat = factor(cat, levels = c("Top 500", "Top 1000", "Top 2000", "Top 0.001%", "Remaining 99.99%"))) %>% 
  head(10500) %>% 
  ggplot(aes(x = rowname, y = volume, fill = cat)) +
  geom_area(alpha = 0.8) +
  theme_light() +
  scale_y_continuous(
    labels = glue("{ylab} M"),
    breaks = 10^6 * ylab,
    limits = c(0, 200* 10^6), 
    expand = c(0,0)
    ) +
  labs(x = "", title = "Long tail") + 
  geom_segment(aes(x = 8000, y = 35*10^6, xend = 10500, yend = 35*10^6),
               arrow = arrow(length = unit(0.5, "cm"))) +
  scale_x_continuous(expand = c(0,0))
```



<br>

```{r search_tails2}
get_count_range <-  function(lower, higher)
{
  sql <- glue(
        "SELECT COUNT(*) AS `count` 
         FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
         WHERE location = 2840 
         AND keyword_info_search_volume >= {lower} 
         AND keyword_info_search_volume <= {higher}")
      tb <- bq_project_query("dataforseo-bigquery", sql)
      bq_table_download(tb)$count
}

df <- tribble(
  ~cat, ~count,
  "0 - 10", get_count_range(0, 10),
  "11- 100", get_count_range(11, 100),
  "101 - 1000", get_count_range(101, 1000),
  "1001 - 10000", get_count_range(1001, 10000),
  "10001 - 100000", get_count_range(10001, "100000"),
  "100001+", get_count_range("100001", "100000000000")) 


df %>% 
  mutate(count = count / sum(count)) %>% 
  ggplot(aes(x = reorder(cat, desc(count)), y = count)) +
  geom_bar(stat = "identity", fill = "turquoise4", color = "black") +
  theme_light() +
  labs(x = "", y = "", title = "Long tail") +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1), expand = c(0,0))
           
```
<br>


Only very small between keyword length and search volume

```{sql, connection = con, output.var = "sql"}
SELECT LENGTH(keyword) as `len_keyword`, keyword_info_search_volume
FROM `dataforseo-bigquery.dataforseo_data.keyword_data`
WHERE location = 2840
AND keyword_info_search_volume IS NOT NULL
ORDER BY RAND()
LIMIT 100000
```


```{r}
cor.test(sql$len_keyword, sql$keyword_info_search_volume)
```



